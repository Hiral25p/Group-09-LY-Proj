{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices: tf.config.experimental.set_memory_growth(device, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout, Activation, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from PIL import Image, ImageChops, ImageEnhance\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Level Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#converts input image to ela applied image\n",
    "def convert_to_ela_image(path,quality):\n",
    "\n",
    "    original_image = Image.open(path).convert('RGB')\n",
    "\n",
    "    #resaving input image at the desired quality\n",
    "    resaved_file_name = 'resaved_image.jpg'     #predefined filename for resaved image\n",
    "    original_image.save(resaved_file_name,'JPEG',quality=quality)\n",
    "    resaved_image = Image.open(resaved_file_name)\n",
    "\n",
    "    #pixel difference between original and resaved image\n",
    "    ela_image = ImageChops.difference(original_image,resaved_image)\n",
    "    \n",
    "    #scaling factors are calculated from pixel extremas\n",
    "    extrema = ela_image.getextrema()\n",
    "    max_difference = max([pix[1] for pix in extrema])\n",
    "    if max_difference ==0:\n",
    "        max_difference = 1\n",
    "    scale = 350.0 / max_difference\n",
    "    \n",
    "    #enhancing elaimage to brighten the pixels\n",
    "    ela_image = ImageEnhance.Brightness(ela_image).enhance(scale)\n",
    "\n",
    "    ela_image.save(\"ela_image.png\")\n",
    "    return ela_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_image(image_path):\n",
    "    image_size = (128, 128)\n",
    "    return np.array(convert_to_ela_image(image_path, 90).resize(image_size)).flatten() / 255.0         #normalizing the array values obtained from input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [] # ELA converted images\n",
    "Y = [] # 0 for fake, 1 for real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define paths to CASIA1 and CASIA2 datasets\n",
    "class Config:\n",
    "    CASIA1 = r\"C:\\Users\\Hiral Patel\\Desktop\\dataset\\CASIA1\"\n",
    "    CASIA2 = r\"C:\\Users\\Hiral Patel\\Desktop\\dataset\\CASIA2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\hiral patel\\appdata\\roaming\\python\\python310\\site-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\hiral patel\\appdata\\roaming\\python\\python310\\site-packages (from ipywidgets) (8.12.3)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\hiral patel\\appdata\\roaming\\python\\python310\\site-packages (from ipywidgets) (5.14.0)\n",
      "Collecting widgetsnbextension~=4.0.12 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.12 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: backcall in c:\\users\\hiral patel\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\hiral patel\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (4.4.2)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\hiral patel\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\hiral patel\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\hiral patel\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\hiral patel\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.39)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\hiral patel\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (2.16.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\hiral patel\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\hiral patel\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\hiral patel\\appdata\\roaming\\python\\python310\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\hiral patel\\appdata\\roaming\\python\\python310\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.12)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\hiral patel\\appdata\\roaming\\python\\python310\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\hiral patel\\appdata\\roaming\\python\\python310\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\hiral patel\\appdata\\roaming\\python\\python310\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\hiral patel\\appdata\\roaming\\python\\python310\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Downloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\n",
      "Downloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.8/2.3 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.8/2.3 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.3/2.3 MB 3.8 MB/s eta 0:00:00\n",
      "Installing collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.5 jupyterlab-widgets-3.0.13 widgetsnbextension-4.0.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 22.3.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install ipywidgets --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Collecting numpy==1.22.0\n",
      "  Downloading numpy-1.22.0-cp310-cp310-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting pandas==1.4\n",
      "  Downloading pandas-1.4.0-cp310-cp310-win_amd64.whl.metadata (12 kB)\n",
      "Collecting scipy==1.11.2\n",
      "  Downloading scipy-1.11.2-cp310-cp310-win_amd64.whl.metadata (59 kB)\n",
      "Collecting pydantic==1.10.2\n",
      "  Downloading pydantic-1.10.2-cp310-cp310-win_amd64.whl.metadata (140 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\hiral patel\\appdata\\roaming\\python\\python310\\site-packages (from pandas==1.4) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hiral patel\\appdata\\roaming\\python\\python310\\site-packages (from pandas==1.4) (2023.3.post1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\hiral patel\\appdata\\roaming\\python\\python310\\site-packages (from pydantic==1.10.2) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hiral patel\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.1->pandas==1.4) (1.16.0)\n",
      "Using cached numpy-1.22.0-cp310-cp310-win_amd64.whl (14.7 MB)\n",
      "Downloading pandas-1.4.0-cp310-cp310-win_amd64.whl (10.6 MB)\n",
      "   ---------------------------------------- 0.0/10.6 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/10.6 MB 4.8 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.8/10.6 MB 4.2 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.6/10.6 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.7/10.6 MB 4.4 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 4.5/10.6 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 5.2/10.6 MB 4.2 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 6.0/10.6 MB 4.1 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 6.8/10.6 MB 4.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.9/10.6 MB 4.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.7/10.6 MB 4.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.4/10.6 MB 4.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.2/10.6 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.6/10.6 MB 3.9 MB/s eta 0:00:00\n",
      "Downloading scipy-1.11.2-cp310-cp310-win_amd64.whl (44.0 MB)\n",
      "   ---------------------------------------- 0.0/44.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.8/44.0 MB 4.8 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 1.8/44.0 MB 4.8 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 2.6/44.0 MB 4.4 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 3.7/44.0 MB 4.3 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 4.5/44.0 MB 4.2 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 5.2/44.0 MB 4.1 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 6.0/44.0 MB 4.1 MB/s eta 0:00:10\n",
      "   ------ --------------------------------- 6.8/44.0 MB 4.1 MB/s eta 0:00:10\n",
      "   ------ --------------------------------- 7.6/44.0 MB 4.0 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 8.4/44.0 MB 4.0 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 9.2/44.0 MB 4.0 MB/s eta 0:00:09\n",
      "   --------- ------------------------------ 10.0/44.0 MB 4.0 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 11.0/44.0 MB 4.0 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 11.8/44.0 MB 4.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 12.6/44.0 MB 4.0 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 13.4/44.0 MB 4.0 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 14.4/44.0 MB 4.0 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 15.2/44.0 MB 4.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 16.0/44.0 MB 4.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 16.8/44.0 MB 4.0 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 17.6/44.0 MB 4.0 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 18.4/44.0 MB 4.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 19.1/44.0 MB 3.9 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 19.9/44.0 MB 3.9 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 21.0/44.0 MB 3.9 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 21.8/44.0 MB 3.9 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 22.5/44.0 MB 3.9 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 23.3/44.0 MB 3.9 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 24.1/44.0 MB 3.9 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 25.2/44.0 MB 3.9 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 26.0/44.0 MB 3.9 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 26.7/44.0 MB 3.9 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 27.5/44.0 MB 3.9 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 28.3/44.0 MB 3.9 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 29.1/44.0 MB 3.9 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 29.9/44.0 MB 3.9 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 30.9/44.0 MB 3.9 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 31.7/44.0 MB 3.9 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 32.8/44.0 MB 3.9 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 33.8/44.0 MB 3.9 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 35.1/44.0 MB 3.9 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 36.2/44.0 MB 3.9 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 37.2/44.0 MB 3.9 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 38.3/44.0 MB 3.9 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 39.1/44.0 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 40.1/44.0 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 41.2/44.0 MB 3.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 41.9/44.0 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.0/44.0 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.8/44.0 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.8/44.0 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.8/44.0 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 44.0/44.0 MB 3.7 MB/s eta 0:00:00\n",
      "Downloading pydantic-1.10.2-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 1.0/2.1 MB 5.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.8/2.1 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 3.9 MB/s eta 0:00:00\n",
      "Installing collected packages: pydantic, numpy, scipy, pandas\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.4\n",
      "    Uninstalling numpy-1.24.4:\n",
      "      Successfully uninstalled numpy-1.24.4\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.10.1\n",
      "    Uninstalling scipy-1.10.1:\n",
      "      Successfully uninstalled scipy-1.10.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.0.3\n",
      "    Uninstalling pandas-2.0.3:\n",
      "      Successfully uninstalled pandas-2.0.3\n",
      "Successfully installed numpy-1.22.0 pandas-1.4.0 pydantic-1.10.2 scipy-1.11.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Hiral Patel\\AppData\\Roaming\\Python\\Python310\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Hiral Patel\\AppData\\Roaming\\Python\\Python310\\site-packages\\~cipy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Hiral Patel\\AppData\\Roaming\\Python\\Python310\\site-packages\\~cipy'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Hiral Patel\\AppData\\Roaming\\Python\\Python310\\site-packages\\~andas'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "chromadb 0.5.4 requires numpy<2.0.0,>=1.22.5, but you have numpy 1.22.0 which is incompatible.\n",
      "gruut 2.2.3 requires networkx<3.0.0,>=2.5.0, but you have networkx 3.0 which is incompatible.\n",
      "mediapipe 0.10.11 requires protobuf<4,>=3.11, but you have protobuf 4.24.4 which is incompatible.\n",
      "spacy 3.5.2 requires typer<0.8.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.12.2 which is incompatible.\n",
      "ultralytics 8.0.200 requires numpy>=1.22.2, but you have numpy 1.22.0 which is incompatible.\n",
      "albucore 0.0.20 requires numpy>=1.24.4, but you have numpy 1.22.0 which is incompatible.\n",
      "albumentations 1.4.21 requires numpy>=1.24.4, but you have numpy 1.22.0 which is incompatible.\n",
      "albumentations 1.4.21 requires pydantic>=2.7.0, but you have pydantic 1.10.2 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 22.3.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install numpy==1.22.0 pandas==1.4 scipy==1.11.2 pydantic==1.10.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in c:\\users\\hiral patel\\appdata\\roaming\\python\\python310\\site-packages (4.66.1)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.0-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\hiral patel\\appdata\\roaming\\python\\python310\\site-packages (from tqdm) (0.4.6)\n",
      "Downloading tqdm-4.67.0-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.1\n",
      "    Uninstalling tqdm-4.66.1:\n",
      "      Successfully uninstalled tqdm-4.66.1\n",
      "Successfully installed tqdm-4.67.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tts 0.22.0 requires numpy==1.22.0; python_version <= \"3.10\", but you have numpy 1.24.4 which is incompatible.\n",
      "tts 0.22.0 requires pandas<2.0,>=1.4, but you have pandas 2.0.3 which is incompatible.\n",
      "tts 0.22.0 requires scipy>=1.11.2, but you have scipy 1.10.1 which is incompatible.\n",
      "spacy 3.5.2 requires pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4, but you have pydantic 2.9.2 which is incompatible.\n",
      "spacy 3.5.2 requires typer<0.8.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 22.3.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27d219d23544817b455c758b5aa20f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Images :   0%|          | 0/7492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 7354\n",
      "Total labels: 7354\n"
     ]
    }
   ],
   "source": [
    "#adding authentic images\n",
    "\n",
    "path = os.path.join(Config.CASIA2, 'Au/')       #folder path of the authentic images in the dataset\n",
    "for filename in tqdm(os.listdir(path),desc=\"Processing Images : \"):\n",
    "    if filename.endswith('jpg') or filename.endswith('png'):\n",
    "        full_path = os.path.join(path, filename)\n",
    "        X.append(prepare_image(full_path))        \n",
    "        Y.append(1)     # label for authentic images \n",
    "        \n",
    "print(f'Total images: {len(X)}\\nTotal labels: {len(Y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42464b251710483cadc658830189648a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Images :   0%|          | 0/5124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 9418\n",
      "Total labels: 9418\n"
     ]
    }
   ],
   "source": [
    "#adding forged images\n",
    "\n",
    "path = os.path.join(Config.CASIA2, 'Tp/')       #folder path of the forged images in the dataset\n",
    "for filename in tqdm(os.listdir(path),desc=\"Processing Images : \"):\n",
    "    if filename.endswith('jpg') or filename.endswith('png'):\n",
    "        full_path = os.path.join(path, filename)\n",
    "        X.append(prepare_image(full_path))        \n",
    "        Y.append(0)     # label for forged images \n",
    "        \n",
    "print(f'Total images: {len(X)}\\nTotal labels: {len(Y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "X = X.reshape(-1, 128, 128, 3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning dataset for training, validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images: 7157 , Training labels: 7157\n",
      "Validation images: 1790 , Validation labels: 1790\n",
      "Test images: 471 , Test labels: 471\n"
     ]
    }
   ],
   "source": [
    "# Training : Validation : Testing = 76 : 19 : 5\n",
    "X_temp, X_test, Y_temp, Y_test = train_test_split(X, Y, test_size = 0.05, random_state=5)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_temp, Y_temp, test_size = 0.2, random_state=5)\n",
    "X = X.reshape(-1,1,1,1)\n",
    "\n",
    "print(f'Training images: {len(X_train)} , Training labels: {len(Y_train)}')\n",
    "print(f'Validation images: {len(X_val)} , Validation labels: {len(Y_val)}')\n",
    "print(f'Test images: {len(X_test)} , Test labels: {len(Y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()  # Sequential Model\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (5, 5), padding = 'valid', activation = 'relu', input_shape = (128, 128, 3)))\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (5, 5), padding = 'valid', activation = 'relu'))\n",
    "    model.add(MaxPool2D(pool_size = (2, 2)))\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (5, 5), padding = 'valid', activation = 'relu'))\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (5, 5), padding = 'valid', activation = 'relu'))\n",
    "    model.add(MaxPool2D(pool_size = (2, 2)))\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (5, 5), padding = 'valid', activation = 'relu'))\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (5, 5), padding = 'valid', activation = 'relu'))\n",
    "    model.add(MaxPool2D(pool_size = (2, 2)))\n",
    "    model.add(Conv2D(filters = 32, kernel_size = (5, 5), padding = 'valid', activation = 'relu'))\n",
    "    model.add(MaxPool2D(pool_size = (2, 2)))\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 124, 124, 64)      4864      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 120, 120, 64)      102464    \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 60, 60, 64)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 56, 56, 64)        102464    \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 52, 52, 64)        102464    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 26, 26, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 22, 22, 64)        102464    \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 18, 18, 64)        102464    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 9, 9, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 5, 5, 32)          51232     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 2, 2, 32)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 32)                0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 568449 (2.17 MB)\n",
      "Trainable params: 568449 (2.17 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 22.3.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: You must give at least one requirement to install (see \"pip help install\")\n"
     ]
    }
   ],
   "source": [
    "pip install --use-deprecated=legacy-resolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "# Initialize learning rate and decay parameters\n",
    "init_lr = 1e-4\n",
    "epochs = 50\n",
    "\n",
    "# Learning rate decay using ExponentialDecay\n",
    "lr_schedule = ExponentialDecay(\n",
    "    initial_learning_rate=init_lr,\n",
    "    decay_steps=epochs,\n",
    "    decay_rate=0.96,  # You can adjust this value\n",
    "    staircase=True)\n",
    "\n",
    "# Create the optimizer with the learning rate schedule\n",
    "optimizer = Adam(learning_rate=lr_schedule)\n",
    "\n",
    "# Compile the model with binary crossentropy loss and accuracy metric\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Early Stopping\n",
    "early_stopping = EarlyStopping(monitor = 'val_accuracy',\n",
    "                               min_delta = 0,\n",
    "                               patience = 10,\n",
    "                               verbose = 0,\n",
    "                               mode = 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "224/224 [==============================] - 2068s 9s/step - loss: 0.3718 - accuracy: 0.8005 - val_loss: 0.2817 - val_accuracy: 0.8581\n",
      "Epoch 2/50\n",
      "224/224 [==============================] - 1780s 8s/step - loss: 0.2731 - accuracy: 0.8747 - val_loss: 0.2572 - val_accuracy: 0.8760\n",
      "Epoch 3/50\n",
      "224/224 [==============================] - 1839s 8s/step - loss: 0.2602 - accuracy: 0.8794 - val_loss: 0.2438 - val_accuracy: 0.8849\n",
      "Epoch 4/50\n",
      "224/224 [==============================] - 1814s 8s/step - loss: 0.2449 - accuracy: 0.8920 - val_loss: 0.2158 - val_accuracy: 0.9101\n",
      "Epoch 5/50\n",
      " 91/224 [===========>..................] - ETA: 16:07 - loss: 0.2353 - accuracy: 0.9011"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train,\n",
    "                 Y_train,\n",
    "                 batch_size = batch_size,\n",
    "                 epochs = epochs,\n",
    "                 validation_data = (X_val, Y_val),\n",
    "                 callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model as a h5 file\n",
    "model.save('.h5') \n",
    "\n",
    "# get the dictionary containing each metric and the loss for each epoch\n",
    "history_dict = hist.history\n",
    "\n",
    "# save it as a json file\n",
    "json.dump(history_dict, open('', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the training and validation curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "#Figure 1\n",
    "ax[0].plot(history_dict['loss'], color='b', label = \"Training loss\")\n",
    "ax[0].plot(history_dict['val_loss'], color='r', label = \"Validation loss\",axes =ax[0])\n",
    "ax[0].set_xlabel('Epochs',fontsize=16)\n",
    "ax[0].set_ylabel('Loss',fontsize=16)\n",
    "legend = ax[0].legend(loc='best', shadow=True)\n",
    "\n",
    "#Figure 2\n",
    "ax[1].plot(history_dict['accuracy'], color='b', label = \"Training accuracy\")\n",
    "ax[1].plot(history_dict['val_accuracy'], color='r',label = \"Validation accuracy\")\n",
    "ax[1].set_xlabel('Epochs',fontsize=16)\n",
    "ax[1].set_ylabel('Accuracy',fontsize=16)\n",
    "legend = ax[1].legend(loc='best', shadow=True)\n",
    "\n",
    "fig.suptitle('Metrics',fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cf_matrix):\n",
    "  \n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in cf_matrix.flatten()] #number of images in each classification block\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)] #percentage value of images in each block w.r.t total images\n",
    "\n",
    "    axes_labels=['Forged', 'Authentic']\n",
    "    labels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    sns.heatmap(cf_matrix, annot=labels, fmt='',cmap=\"flare\" , xticklabels=axes_labels, yticklabels=axes_labels)\n",
    "\n",
    "    plot_xlabel = plt.xlabel('Predicted labels', fontsize = 13)\n",
    "    plot_ylabel = plt.ylabel('True labels', fontsize = 13)\n",
    "    plot_title = plt.title('Confusion Matrix', fontsize= 10,fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_val)               # Predict the values from the validation dataset \n",
    "Y_pred_classes = np.round(Y_pred)           # roundoff the sigmoid value\n",
    "Y_true = Y_val                             \n",
    "\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes)     # compute the confusion matrix\n",
    "plot_confusion_matrix(confusion_mtx)                         # plot the confusion matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_true, Y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['Forged', 'Authentic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing accuracy\n",
    "correct_test = 0 #correctly predicted test images\n",
    "total_test = 0   #total test images\n",
    "\n",
    "for index,image in enumerate(tqdm(X_test,desc=\"Processing Images : \")):\n",
    "    image = image.reshape(-1, 128, 128, 3)\n",
    "    y_pred = model.predict(image)\n",
    "    y_pred_class = np.round(y_pred)\n",
    "    total_test += 1\n",
    "    if y_pred_class == Y_test[index]: #if prediction is correct\n",
    "        correct_test += 1\n",
    "    \n",
    "print(f'Total test images: {total_test}\\nCorrectly predicted images: {correct_test}\\nAccuracy: {correct_test / total_test * 100.0} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_path = ''    # test image path\n",
    "test_image = prepare_image(test_image_path)\n",
    "test_image = test_image.reshape(-1, 128, 128, 3)\n",
    "\n",
    "y_pred = model.predict(test_image)\n",
    "y_pred_class = round(y_pred[0][0])\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(15,5)) \n",
    "\n",
    "#display original image\n",
    "original_image = plt.imread(test_image_path) \n",
    "ax[0].axis('off')\n",
    "ax[0].imshow(original_image)\n",
    "ax[0].set_title('Original Image')\n",
    "\n",
    "#display ELA applied image\n",
    "ax[1].axis('off')\n",
    "ax[1].imshow(convert_to_ela_image(test_image_path,90)) \n",
    "ax[1].set_title('ELA Image')\n",
    "\n",
    "print(f'Prediction: {class_names[y_pred_class]}')\n",
    "if y_pred<=0.5:\n",
    "    print(f'Confidence:  {(1-(y_pred[0][0])) * 100:0.2f}%')\n",
    "else:\n",
    "    print(f'Confidence: {(y_pred[0][0]) * 100:0.2f}%')\n",
    "print('--------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folder_path = ''        #dataset path\n",
    "authentic,forged,total = 0,0,0\n",
    "\n",
    "for filename in tqdm(os.listdir(test_folder_path),desc=\"Processing Images : \"):\n",
    "    if filename.endswith('jpg') or filename.endswith('png'):\n",
    "        test_image_path = os.path.join(path, filename)\n",
    "        test_image = prepare_image(test_image_path)  \n",
    "        test_image.reshape(-1, 128, 128, 3)\n",
    "        y_pred = model.predict(image)\n",
    "        y_pred_class = np.round(y_pred)\n",
    "        total += 1\n",
    "        if y_pred_class == 0:\n",
    "            forged += 1\n",
    "        else:\n",
    "            authentic +=1\n",
    "\n",
    "print(f'Total images: {total}\\nAuthentic Images: {authentic}\\nForged Images: {forged}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
